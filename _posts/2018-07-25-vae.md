---
layout: poster_list
title: 	Tutorial on Variational Autoencoders
categories: paper_reading
src: /blogs/20180725_vae_imgs
en: /paper_reading/2018/07/25/vae
cn: /paper_reading_cn/2018/07/25/vae-cn
---

<h2 align="center">{{page.title}}</h2>
<p>Variational autoencoder (VAE) is a popular generative model proposed recently [1][2]. This poster gives a simple explanation of VAE and is inspired by the <a href="https://arxiv.org/pdf/1606.05908.pdf" target="_blank">tutorial</a>[3]. I will start from the big picture of generative and go to the details of VAE step by step. Hope this poster makes sense to you.</p>

<h3>1. Outline</h3>
<ul>
    <li><a href="#background">Background</a></li>
    <li><a href="#vae">VAE Model</a></li>
    <li><a href="#cvae">Conditional VAE (CVAE) Model</a></li>
    <li><a href="#experiments">Experiments</a></li>
</ul>

<h3 id="background">2. Background</h3>
<p>In the realm of machine learning, there are basically two kinds of models: the discriminative model and the generative model. Suppose we have an observable variable $X$ and a target variable $Y$. Usually $X$ can be the variable representing an input image and $Y$ stands for the label of the image. A discriminative model tries to predict the target $Y$ given the observed variable $X$. It models the conditional probability $p(Y|X=x)$. However, a generative model tries to learn how the data is generated given a certain target (or label) $Y=y$. In other words, it models the probability $p(X|Y=y)$. Sometimes even the target is not given. The model tries to capture the distribution of the observable variable $X$, <emph>i.e.</emph> $p(X)$.</p>

<p>In a traditional generative model like latent Dirichlet allocation (LDA) [4], we usually design how the data is generated by hand. Take LDA for example, it tries to generates a bunch of documents. We first have a topic distribution. Each topic defines a probability distribution over the words. Such a generation procedure is designed by hand. These traditional generative models have at least three draw-backs:
<ul>
    <li>Strong assumptions about the structure of the data;</li>
    <li>Severe approximations which lead to suboptimal problems;</li>
    <li>Computational expensive inference like MCMC.</li>
</ul>
VAE is also a kind of generative models with very simple assumptions and yet is very powerful.</p>

<h3 id="vae">2. VAE Model</h3>
<h4>2.1 Latent Variable Model</h4>
<p>The graphical model of VAE is shown in Figure. 1. Assume there is a latent variable $Z$ that contains all the information needed to generate a sample. For example, for a digit generation task, one dimension of $Z$ can represent what the digit is while another dimension can stand for the font of the digit. With the information given </p>

<div align="center">
  <img width="100%" src="{{page.src}}/graphical_model.png"/>
  <div class="caption">Figure 1. Graphical Model of VAE.</div>
</div>

<h4 id="reference">Reference</h4>
[1] Kingma D P, Welling M. Auto-encoding variational bayes[J]. arXiv preprint arXiv:1312.6114, 2013. <br/>
[2] Rezende D J, Mohamed S, Wierstra D. Stochastic backpropagation and approximate inference in deep generative models[J]. arXiv preprint arXiv:1401.4082, 2014. <br/>
[3] Doersch C. Tutorial on variational autoencoders[J]. arXiv preprint arXiv:1606.05908, 2016. <br/>
[4] Blei D M, Ng A Y, Jordan M I. Latent dirichlet allocation[J]. Journal of machine Learning research, 2003, 3(Jan): 993-1022. <br/>